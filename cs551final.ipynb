{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs551final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "O-eN5C-fo8D3",
        "zwG_sc-rpCdf",
        "T0igDEa2ovyT",
        "N_rVpC4wo1r8",
        "U6bPWgN9puD9",
        "V0e_Qey2qhDR",
        "9Tpks0XqrJ1d"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "taJS0ui_M1L0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **CS551 Final Project** \n",
        "\n",
        "Mathieu Rundström, Shenshun Yao, and Yuxiang Ma"
      ]
    },
    {
      "metadata": {
        "id": "O-eN5C-fo8D3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utility"
      ]
    },
    {
      "metadata": {
        "id": "rskcf_Kvj0sr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YpET7Y9vkLzO",
        "colab_type": "code",
        "outputId": "aac7367c-14a8-41f1-a36f-c8b95a72e2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UJsCdbCYl1WU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TOTrainParameter(object):\n",
        "    \n",
        "    def __init__(self, train_file, num_class=2, embedding_dim=100, \n",
        "                 filter_sizes=\"3,4,5\", num_filters=100, dropout_keep_prob=0.5, \n",
        "                 l2_reg_lambda=0.0, l2_reg_constraint=3.0, batch_size=50, \n",
        "                 num_epochs=25, num_checkpoint=5, learn_rate=0.001):\n",
        "        self.num_class = num_class\n",
        "        self.train_file = train_file\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.filter_sizes = filter_sizes\n",
        "        self.num_filters = num_filters\n",
        "        self.dropout_keep_prob = dropout_keep_prob\n",
        "        self.l2_reg_lambda = l2_reg_lambda\n",
        "        self.l2_reg_constraint = l2_reg_constraint\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.num_checkpoints = num_checkpoint\n",
        "        self.learn_rate = learn_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kc4s718-qMCA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import operator\n",
        "\n",
        "\n",
        "def fold_left(func, acc, xs):\n",
        "    return functools.reduce(func, xs, acc)\n",
        "\n",
        "to_trainer = TOTrainParameter(\n",
        "    '/content/gdrive/My Drive/cs551final/data'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwG_sc-rpCdf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "H2n5TDFikpQB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import collections\n",
        "import numpy as np\n",
        "import gensim.models\n",
        "from tensorflow import logging\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "def parse_mr(positive_data_file, negative_data_file):\n",
        "    \"\"\"\n",
        "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
        "    Returns split sentences and labels.\n",
        "    \"\"\"\n",
        "    # Load data from files\n",
        "    positive_examples = list(open(positive_data_file, \"r\", encoding='utf-8').readlines())\n",
        "    positive_examples = [clean_str(s.strip()) for s in positive_examples]\n",
        "    negative_examples = list(open(negative_data_file, \"r\", encoding='utf-8').readlines())\n",
        "    negative_examples = [clean_str(s.strip()) for s in negative_examples]\n",
        "    \n",
        "    # Generate labels\n",
        "    positive_labels = [[0, 1] for _ in positive_examples]\n",
        "    negative_labels = [[1, 0] for _ in negative_examples]\n",
        "    \n",
        "    return positive_examples, positive_labels, negative_examples, negative_labels\n",
        "\n",
        "\n",
        "def parse_sst1(dir):\n",
        "\n",
        "    index2split = {}\n",
        "    with open(dir + '/stanfordSentimentTreebank/datasetSplit.txt') as train_test_validation_split:\n",
        "        for line_split in train_test_validation_split.readlines()[1:]:\n",
        "            index2split[int(line_split.split(',')[0])] = int(line_split.split(',')[1])\n",
        "\n",
        "    index2class = {}\n",
        "    with open(dir + '/stanfordSentimentTreebank/sentiment_labels.txt') as sent_classes:\n",
        "        for sent_class in sent_classes.readlines()[1:]:\n",
        "            rate = float(sent_class.split('|')[1])\n",
        "            index = int(sent_class.split('|')[0])\n",
        "            if 0 <= rate <= 0.2:\n",
        "                index2class[index] = [1, 0, 0, 0, 0]  # 0\n",
        "            elif 0.2 < rate <= 0.4:\n",
        "                index2class[index] = [0, 1, 0, 0, 0]  # 1\n",
        "            elif 0.4 < rate <= 0.6:\n",
        "                index2class[index] = [0, 0, 1, 0, 0]  # 2\n",
        "            elif 0.6 < rate <= 0.8:\n",
        "                index2class[index] = [0, 0, 0, 1, 0]  # 3\n",
        "            elif 0.8 < rate <= 1.0:\n",
        "                index2class[index] = [0, 0, 0, 0, 1]  # 4\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    x_validation = []\n",
        "    y_validation = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    vocab = collections.defaultdict(float)\n",
        "    with open(dir + '/stanfordSentimentTreebank/datasetSentences.txt') as sentences:\n",
        "        for line in sentences.readlines()[1:]:\n",
        "            sentence_split = line.split()\n",
        "            index = int(sentence_split[0])\n",
        "            text = clean_str(\" \".join(sentence_split[1:]))\n",
        "            if index2split[index] == 1:\n",
        "                x_train.append(text)\n",
        "                y_train.append(index2class[index])\n",
        "                for word in set(text.split()):\n",
        "                    vocab[word] += 1\n",
        "            elif index2split[index] == 2:\n",
        "                x_test.append(text)\n",
        "                y_test.append(index2class[index])\n",
        "                for word in set(text.split()):\n",
        "                    vocab[word] += 1\n",
        "            elif index2split[index] == 3:\n",
        "                x_validation.append(text)\n",
        "                y_validation.append(index2class[index])\n",
        "                for word in set(text.split()):\n",
        "                    vocab[word] += 1\n",
        "\n",
        "    return x_train, y_train, x_validation, y_validation, x_test, y_test, vocab\n",
        "\n",
        "\n",
        "def parse_sst2(dir):\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    x_validation = []\n",
        "    y_validation = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    vocab = collections.defaultdict(float)\n",
        "    with open(dir + '/stanfordSentimentTreebank/stsa.binary.dev') as validation_set:\n",
        "        for validation in validation_set.readlines():\n",
        "            spl = validation.split()\n",
        "            text = \" \".join(spl[1:])\n",
        "            if int(spl[0]) == 1:\n",
        "                y_validation.append([0, 1])\n",
        "            else:\n",
        "                y_validation.append([1, 0])\n",
        "            x_validation.append(text)\n",
        "            for word in set(text.split()):\n",
        "                vocab[word] += 1\n",
        "\n",
        "    with open(dir + '/stanfordSentimentTreebank/stsa.binary.test') as test_set:\n",
        "        for test in test_set.readlines():\n",
        "            spl = test.split()\n",
        "            text = \" \".join(spl[1:])\n",
        "            if int(spl[0]) == 1:\n",
        "                y_test.append([0, 1])\n",
        "            else:\n",
        "                y_test.append([1, 0])\n",
        "            x_test.append(text)\n",
        "            for word in set(text.split()):\n",
        "                vocab[word] += 1\n",
        "\n",
        "    with open(dir + '/stanfordSentimentTreebank/stsa.binary.train') as train_set:\n",
        "        for train in train_set.readlines():\n",
        "            spl = train.split()\n",
        "            text = clean_str(\" \".join(spl[1:]))\n",
        "            if int(spl[0]) == 1:\n",
        "                y_train.append([0, 1])\n",
        "            else:\n",
        "                y_train.append([1, 0])\n",
        "            x_train.append(text)\n",
        "            for word in set(text.split()):\n",
        "                vocab[word] += 1\n",
        "\n",
        "    return x_train, y_train, x_validation, y_validation, x_test, y_test, vocab\n",
        "\n",
        "\n",
        "def parse_trec(dir):\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    x_validation = []\n",
        "    y_validation = []\n",
        "\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "\n",
        "    label2class = {\"ABBR\": [1, 0, 0, 0, 0, 0], \"ENTY\": [0, 1, 0, 0, 0, 0], \"DESC\": [0, 0, 1, 0, 0, 0],\n",
        "                   \"HUM\":  [0, 0, 0, 1, 0, 0], \"LOC\":  [0, 0, 0, 0, 1, 0], \"NUM\":  [0, 0, 0, 0, 0, 1]}\n",
        "\n",
        "    vocab = collections.defaultdict(float)\n",
        "    with open(dir + '/TREC/train_1000.label', encoding=\"ISO-8859-1\") as test_set:\n",
        "        for test in test_set.readlines():\n",
        "            spl = test.split(\" \")\n",
        "            text = clean_str(\" \".join(spl[1:]))\n",
        "            y_train.append(label2class[spl[0].split(\":\")[0]])\n",
        "            x_train.append(text)\n",
        "            for word in set(text.split()):\n",
        "                vocab[word] += 1\n",
        "\n",
        "    with open(dir + '/TREC/train_2000.label', encoding=\"ISO-8859-1\") as test_set:\n",
        "        for test in test_set.readlines():\n",
        "            spl = test.split(\" \")\n",
        "            text = clean_str(\" \".join(spl[1:]))\n",
        "            y_train.append(label2class[spl[0].split(\":\")[0]])\n",
        "            x_train.append(text)\n",
        "            for word in set(text.split()):\n",
        "                vocab[word] += 1\n",
        "\n",
        "    with open(dir + '/TREC/train_3000.label', encoding=\"ISO-8859-1\") as test_set:\n",
        "        for test in test_set.readlines():\n",
        "            spl = test.split(\" \")\n",
        "            text = clean_str(\" \".join(spl[1:]))\n",
        "            y_train.append(label2class[spl[0].split(\":\")[0]])\n",
        "            x_train.append(text)\n",
        "            for word in set(text.split()):\n",
        "                vocab[word] += 1\n",
        "\n",
        "    with open(dir + '/TREC/train_4000.label', encoding=\"ISO-8859-1\") as test_set:\n",
        "        for test in test_set.readlines():\n",
        "            spl = test.split(\" \")\n",
        "            text = clean_str(\" \".join(spl[1:]))\n",
        "            y_train.append(label2class[spl[0].split(\":\")[0]])\n",
        "            x_train.append(text)\n",
        "            for word in set(text.split()):\n",
        "                vocab[word] += 1\n",
        "\n",
        "    with open(dir + '/TREC/train_5500.label', encoding=\"ISO-8859-1\") as test_set:\n",
        "        for test in test_set.readlines():\n",
        "            spl = test.split(\" \")\n",
        "            text = clean_str(\" \".join(spl[1:]))\n",
        "            y_validation.append(label2class[spl[0].split(\":\")[0]])\n",
        "            x_validation.append(text)\n",
        "            for word in set(text.split()):\n",
        "                vocab[word] += 1\n",
        "\n",
        "    with open(dir + '/TREC/TREC_10.label', encoding=\"ISO-8859-1\") as test_set:\n",
        "        for test in test_set.readlines():\n",
        "            spl = test.split(\" \")\n",
        "            text = clean_str(\" \".join(spl[1:]))\n",
        "            y_test.append(label2class[spl[0].split(\":\")[0]])\n",
        "            x_test.append(text)\n",
        "            for word in set(text.split()):\n",
        "                vocab[word] += 1\n",
        "\n",
        "    return x_train, y_train, x_validation, y_validation, x_test, y_test, vocab\n",
        "\n",
        "\n",
        "def transform_google_w2v(x_text_vec, embd_dim, dir):\n",
        "\n",
        "    w2v = gensim.models.KeyedVectors.load_word2vec_format(dir + '/W2V/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "    vocab = []\n",
        "    embedding = []\n",
        "    curr = 0\n",
        "    s = set((\" \".join(x_text_vec)).split())\n",
        "    \n",
        "    for k, v in w2v.vocab.items():\n",
        "        if k in s:\n",
        "            vocab.append(str(k))\n",
        "            embedding.append(list(w2v[k])[:embd_dim])\n",
        "            curr = curr + 1\n",
        "    vocab_lookup = set(vocab)\n",
        "    \n",
        "    for text in x_text_vec:\n",
        "        for word in text.split():\n",
        "            if word not in vocab_lookup:\n",
        "                vocab.append(word)\n",
        "                embedding.append(list(np.random.uniform(-0.25, 0.25, embd_dim)))\n",
        "                curr = curr + 1\n",
        "\n",
        "    return vocab, embedding\n",
        "\n",
        "\n",
        "def transform_random_w2v(x_text_vec):\n",
        "    \n",
        "    vocab = []\n",
        "    embedding = []\n",
        "    curr = 0\n",
        "    for text in x_text_vec:\n",
        "        for word in text.split():\n",
        "            vocab.append(word)\n",
        "            embedding.append(list(np.random.uniform(-0.25, 0.25, embd_dim)))\n",
        "            curr = curr + 1\n",
        "\n",
        "    return vocab, embedding\n",
        "\n",
        "\n",
        "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates a batch iterator for a dataset.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T0igDEa2ovyT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN Class Model"
      ]
    },
    {
      "metadata": {
        "id": "gqGu7H0_lBc5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class InvalidInputException(Exception): \n",
        "    \"\"\"傻逼Gunter的垃圾Class\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class CNNText:\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, embedding_vec, filter_sizes, num_filters, l2_reg_lambda=0.0, l2_reg_constrains=3, is_static=1):\n",
        "\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        # Keeping track of l2 regularization loss (optional)\n",
        "        l2_loss = tf.constant(0.0)\n",
        "\n",
        "        # Embedding layer\n",
        "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "            \n",
        "            if is_static == 1: \n",
        "                num_channel = 1\n",
        "                self.W = tf.Variable(\n",
        "                    tf.random_uniform([vocab_size, embedding_size], -0.25, 0.25),\n",
        "                    name=\"W\", \n",
        "                    trainable=True\n",
        "                )\n",
        "            elif is_static == 2 or is_static == 3:\n",
        "                num_channel = 1\n",
        "                self.W = tf.Variable(\n",
        "                    embedding_vec,\n",
        "                    name=\"W\", \n",
        "                    validate_shape = False, \n",
        "                    trainable=(is_static == 3)\n",
        "                )\n",
        "                \n",
        "            else: \n",
        "                num_channel = 2\n",
        "                self.W = tf.Variable(\n",
        "                    embedding_vec,\n",
        "                    name=\"W\", \n",
        "                    validate_shape = False, \n",
        "                    trainable=True\n",
        "                )\n",
        "                self.W_static = tf.Variable(\n",
        "                    embedding_vec,\n",
        "                    name=\"W_static\", \n",
        "                    validate_shape = False, \n",
        "                    trainable=False\n",
        "                )\n",
        "            \n",
        "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "            \n",
        "            if is_static == 4: \n",
        "                \n",
        "                self.embedded_chars_static = tf.nn.embedding_lookup(self.W_static, self.input_x)\n",
        "                self.embedded_chars_expanded_static = tf.expand_dims(self.embedded_chars_static, -1)\n",
        "                self.embedded_chars_expanded = tf.concat(\n",
        "                    [\n",
        "                        self.embedded_chars_expanded, \n",
        "                        self.embedded_chars_expanded_static\n",
        "                    ], \n",
        "                    axis=-1\n",
        "                )\n",
        "                \n",
        "\n",
        "        # Create a convolution + maxpool layer for each filter size\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size, num_channel, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                    \n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.embedded_chars_expanded,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\"\n",
        "                )\n",
        "                # Apply nonlinearity\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\"\n",
        "                )\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer()\n",
        "            )\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "            l2_loss += tf.nn.l2_loss(W)\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "        # Calculate mean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_rVpC4wo1r8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "bG281O5xljO3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "import gensim\n",
        "\n",
        "\n",
        "def train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_train_param, dir_name, is_static=1):\n",
        "\n",
        "    # Build vocabulary\n",
        "    whole_doc = x_train + x_validation\n",
        "    max_document_length = max([len(x.split(\" \")) for x in whole_doc])\n",
        "    \n",
        "    print(\"Generating Word2Vec--\")\n",
        "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    if is_static == 1: \n",
        "        pretrain_rand = vocab_processor.fit(fold_left(operator.add, [], [x.split(\" \") for x in whole_doc]))\n",
        "        embedding = [[]]\n",
        "    elif is_static == 2 or is_static == 3 or is_static == 4: \n",
        "        vocab, embedding = transform_google_w2v(whole_doc, to_train_param.embedding_dim, to_train_param.train_file)\n",
        "        pretrain = vocab_processor.fit(vocab)\n",
        "    else: \n",
        "        raise InvalidInputException(\"Is static should be from 1-3.\")\n",
        "    \n",
        "    print(\"Transform\")\n",
        "    x_vec_index_train = np.array(list(vocab_processor.transform(x_train)))\n",
        "    x_vec_index_validation = np.array(list(vocab_processor.transform(x_validation)))\n",
        "    x_vec_index_test = np.array(list(vocab_processor.transform(x_test)))\n",
        "    \n",
        "    y_train = np.array(y_train)\n",
        "    y_validation = np.array(y_validation)\n",
        "    y_test = y_test\n",
        "    print(len(y_test))\n",
        "    print(len(x_vec_index_test))\n",
        "    print(\"Start Session--\")\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        session_conf = tf.ConfigProto(\n",
        "            allow_soft_placement=True,\n",
        "            log_device_placement=False\n",
        "        )\n",
        "        sess = tf.Session(config=session_conf)\n",
        "        with sess.as_default():\n",
        "            cnn = CNNText(\n",
        "                sequence_length=x_vec_index_train.shape[1],\n",
        "                num_classes=y_train.shape[1],\n",
        "                vocab_size=len(vocab_processor.vocabulary_),\n",
        "                embedding_size=to_train_param.embedding_dim,\n",
        "                embedding_vec=embedding, \n",
        "                filter_sizes=list(map(int, to_train_param.filter_sizes.split(\",\"))),\n",
        "                num_filters=to_train_param.num_filters,\n",
        "                l2_reg_lambda=to_train_param.l2_reg_lambda,\n",
        "                is_static=is_static,\n",
        "            )\n",
        "\n",
        "            # Define Training procedure\n",
        "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "            \n",
        "            # learn_rate = tf.train.exponential_decay(to_train_param.learn_rate, global_step, 100, 0.001, staircase=True)\n",
        "            optimizer = tf.train.AdadeltaOptimizer(to_train_param.learn_rate, rho=0.95, epsilon=1e-06)\n",
        "            # optimizer = tf.train.AdamOptimizer(to_train_param.learn_rate)\n",
        "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "            \n",
        "            # Gradient clipping by norm\n",
        "            capped_grads_and_vars = [(tf.clip_by_norm(gv[0], to_train_param.l2_reg_constraint, axes=[0]), gv[1]) for gv in grads_and_vars]\n",
        "            train_op = optimizer.apply_gradients(capped_grads_and_vars, global_step=global_step)\n",
        "            \n",
        "            # Keep track of gradient values and sparsity (optional)\n",
        "            grad_summaries = []\n",
        "            for g, v in grads_and_vars:\n",
        "                if g is not None:\n",
        "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                    grad_summaries.append(grad_hist_summary)\n",
        "                    grad_summaries.append(sparsity_summary)\n",
        "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "            \n",
        "            # Output directory for models and summaries\n",
        "            timestamp = str(int(time.time()))\n",
        "            out_dir = os.path.abspath(os.path.join('/content/gdrive/My Drive/cs551final', dir_name, timestamp))\n",
        "            print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "            # Summaries for loss and accuracy\n",
        "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "            # Train Summaries\n",
        "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "            # Dev summaries\n",
        "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "            # Initialize all variables\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model.ckpt\")\n",
        "            if not os.path.exists(checkpoint_dir):\n",
        "                os.makedirs(checkpoint_dir)\n",
        "            saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V1)\n",
        "            # saver = tf.train.Saver({'embedding/W:0': cnn.W})\n",
        "            # Write vocabulary\n",
        "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "            \n",
        "            vocab_dict = {v: k for k, v in vocab_processor.vocabulary_._mapping.items()}\n",
        "            with open(os.path.join(checkpoint_dir, 'embeddings.tsv'), 'w') as file_1:\n",
        "                for i in range(len(vocab_processor.vocabulary_)):\n",
        "                    word = vocab_dict[i]\n",
        "                    file_1.write('%s\\n' % (word))             \n",
        "            \n",
        "            config = projector.ProjectorConfig()\n",
        "            embed = config.embeddings.add()\n",
        "            embed.tensor_name = \"embedding/W:0\"\n",
        "            embed.metadata_path = os.path.join(checkpoint_dir, 'embeddings.tsv')\n",
        "            ckpt_writer = tf.summary.FileWriter(checkpoint_dir + '/emb_viz_%s.log' % 1, sess.graph)\n",
        "            projector.visualize_embeddings(ckpt_writer, config)\n",
        "             \n",
        "            \n",
        "            \n",
        "\n",
        "            def train_step(x_batch, y_batch, num):\n",
        "                \"\"\"\n",
        "                A single training step\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                    cnn.input_x: x_batch,\n",
        "                    cnn.input_y: y_batch,\n",
        "                    cnn.dropout_keep_prob: to_train_param.dropout_keep_prob\n",
        "                }\n",
        "                _, step, summaries, loss, accuracy = sess.run(\n",
        "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
        "                    feed_dict\n",
        "                )\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                if num % (int((len(y_train)-1)/to_train_param.batch_size) + 1) == 0: \n",
        "                    print(\"\\nTrain:\" + str(num / (int((len(y_train)-1)/to_train_param.batch_size + 1))))\n",
        "                    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "                train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "            def validation_step(x_batch, y_batch):\n",
        "                \"\"\"\n",
        "                Evaluates model on a dev set\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                    cnn.input_x: x_batch,\n",
        "                    cnn.input_y: y_batch,\n",
        "                    cnn.dropout_keep_prob: to_train_param.dropout_keep_prob\n",
        "                }\n",
        "                step, summaries, loss, accuracy = sess.run(\n",
        "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
        "                    feed_dict)\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "                dev_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "            # Generate batches\n",
        "            print(\"Generate Batch\")\n",
        "            batches = batch_iter(\n",
        "                list(zip(x_vec_index_train, y_train)), \n",
        "                to_train_param.batch_size, \n",
        "                to_train_param.num_epochs\n",
        "            )\n",
        "\n",
        "            # Training loop. For each batch...\n",
        "            prev = 0\n",
        "            for batch in batches:\n",
        "                x_batch, y_batch = zip(*batch)\n",
        "                train_step(x_batch, y_batch, prev)\n",
        "                current_step = tf.train.global_step(sess, global_step)\n",
        "                prev = current_step\n",
        "                if current_step % (int((len(y_train)-1)/to_train_param.batch_size) + 1) == 0:\n",
        "                    print(\"\\nEvaluation:\" + str(current_step / (int((len(y_train)-1)/to_train_param.batch_size + 1))))\n",
        "                    validation_step(x_vec_index_validation, y_validation)\n",
        "                    print(\"\")\n",
        "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            train_summary_writer.close()\n",
        "            dev_summary_writer.close()\n",
        "            ckpt_writer.close()\n",
        "            # Get the placeholders from the graph by name\n",
        "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
        "\n",
        "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
        "            \n",
        "            # Reset W in fully connected layer to be scaled by dropout probability during evaluation\n",
        "            W_0 = graph.get_tensor_by_name(\"W:0\")\n",
        "            W_0 = tf.assign(W_0, tf.math.multiply(tf.constant(to_train_param.dropout_keep_prob, dtype=tf.float32), W_0))\n",
        "            \n",
        "            # Tensors we want to evaluate\n",
        "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
        "\n",
        "            # Generate batches for one epoch\n",
        "            batches = batch_iter(list(x_vec_index_test), 50, 1, shuffle=False)\n",
        "\n",
        "            # Collect the predictions here\n",
        "            all_predictions = []\n",
        "\n",
        "            for x_test_batch in batches:\n",
        "                batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
        "                all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
        "    \n",
        "    print(len(all_predictions))\n",
        "    \n",
        "    # Print accuracy if y_test is defined\n",
        "    if y_test is not None:\n",
        "        correct_predictions = 0\n",
        "        for r in range(len(y_test)): \n",
        "            this_pred = [0 for _ in range(y_train.shape[1])]\n",
        "            index = int(all_predictions[r])\n",
        "            this_pred[index] = 1\n",
        "            if y_test[r] == this_pred: \n",
        "                correct_predictions = correct_predictions + 1\n",
        "        print(\"Total number of test examples: {}\".format(len(y_test)))\n",
        "        print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYTm_bCjb2ib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Before Running your section, do a \"Runtime\" -> \"Run before\" on previous cells. "
      ]
    },
    {
      "metadata": {
        "id": "U6bPWgN9puD9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SST-1"
      ]
    },
    {
      "metadata": {
        "id": "RnIEY9ROlrFu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SST1 Adadelta Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "oCppwEfKmids",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST1 Multi Channel"
      ]
    },
    {
      "metadata": {
        "id": "Q7Z5s01-mhrg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 5\n",
        "to_trainer.num_epochs = 5\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst1(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-1 Multi Channel\" , is_static=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OAem9IaCmK52",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST1 non static"
      ]
    },
    {
      "metadata": {
        "id": "oKyutWXPpthl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 5\n",
        "to_trainer.num_epochs = 2\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst1(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-1 Non-static\" , is_static=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kYFljmWxmT7b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST1 static"
      ]
    },
    {
      "metadata": {
        "id": "tcuA2QcambfB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 5\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst1(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-1 Static\" , is_static=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tvy_NaLmdV0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST1 random"
      ]
    },
    {
      "metadata": {
        "id": "_TjXUuBGmfBf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 5\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst1(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-1 Random\" , is_static=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2RLVVbPksBe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SST1 Adam Optimizer\n",
        "change section Train and Evaluation, ```train``` function \n",
        "```\n",
        "optimizer = tf.train.AdadeltaOptimizer(to_train_param.learn_rate, rho=0.95, epsilon=1e-06)\n",
        "```\n",
        "to \n",
        "```\n",
        "optimizer = tf.train.AdamOptimizer(to_train_param.learn_rate)\n",
        "```\n",
        "to use Adam Optimizer instead of Adadelta Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "SCa7I_IMko19",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST1 Random "
      ]
    },
    {
      "metadata": {
        "id": "hkUG8ZDtlgiO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 5\n",
        "to_trainer.num_epochs = 5\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 0.001\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst1(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-1 Random Adam\" , is_static=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-aG4uYLql3Sz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST1 Static"
      ]
    },
    {
      "metadata": {
        "id": "gmfKainkl-qO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 5\n",
        "to_trainer.num_epochs = 5\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 0.001\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst1(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-1 Static Adam\" , is_static=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlUBZmrRmD2G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST1 Non Static"
      ]
    },
    {
      "metadata": {
        "id": "GhgsEBMVmFkx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 5\n",
        "to_trainer.num_epochs = 5\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 0.001\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst1(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-1 Static Adam\" , is_static=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b7YugxE-mH2f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST1 Multi Channel"
      ]
    },
    {
      "metadata": {
        "id": "Cjb6zjbwmJZL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 5\n",
        "to_trainer.num_epochs = 5\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 0.001\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst1(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-1 Multi Channel Adam\" , is_static=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0e_Qey2qhDR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SST-2"
      ]
    },
    {
      "metadata": {
        "id": "Iq2Cqtj8mnwN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST2 Multi Channel"
      ]
    },
    {
      "metadata": {
        "id": "oLuWLlNuO2VL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 2\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst2(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-2 multi-channel\" , is_static=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EEl_3hu7mpyC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST2 Non static"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FmnmipzvbVAI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 2\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst2(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-2 non-static\" , is_static=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3F3clqoamvpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST2 Static "
      ]
    },
    {
      "metadata": {
        "id": "h2kMlJGFh62j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 2\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 100\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst2(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation,  x_test, y_test, to_trainer, \"SST-2 static\" , is_static=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_TlcV9QnYZG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SST2 Random"
      ]
    },
    {
      "metadata": {
        "id": "RFfLmCEwOdBH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 2\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_sst2(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"SST-2 rand\" , is_static=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Tpks0XqrJ1d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##TREC"
      ]
    },
    {
      "metadata": {
        "id": "SNeBaSrNneCY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TREC Random"
      ]
    },
    {
      "metadata": {
        "id": "72tXldm-rNTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 2\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0.0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_trec(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"TREC Random\", is_static=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IZgB0U7In_9K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TREC Static "
      ]
    },
    {
      "metadata": {
        "id": "NjQH_21Cn5OQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 2\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_trec(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"TREC\", is_static=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2wkLmKVyoD38",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TREC Non-Static"
      ]
    },
    {
      "metadata": {
        "id": "WHT7A-3gn5WX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 2\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_trec(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"TREC\", is_static=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0WtyN1YloGXw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TREC Multi-Channel"
      ]
    },
    {
      "metadata": {
        "id": "YqywBdBUn5a9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_trainer.num_class = 2\n",
        "to_trainer.num_epochs = 25\n",
        "to_trainer.batch_size = 50\n",
        "to_trainer.num_filters = 100\n",
        "to_trainer.embedding_dim = 300\n",
        "to_trainer.l2_reg_lambda = 0\n",
        "to_trainer.learn_rate = 1\n",
        "to_trainer.l2_reg_constraint = 3.0\n",
        "\n",
        "x_train, y_train, x_validation, y_validation, x_test, y_test, vocab = parse_trec(to_trainer.train_file)\n",
        "\n",
        "train(x_train, y_train, x_validation, y_validation, x_test, y_test, to_trainer, \"TREC\", is_static=4)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}